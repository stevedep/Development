---
title: "Milestone Report - Capstone"
author: "Steve de Peijper"
date: "26 november 2016"
output: html_document
---

The document describes the intermediate results of the capstone project. In this project text data is loaded, analyzed and used to build a test prediction model. 

This document consists of:
1. loading the data
2. exploring the data
3. next steps

#Loading the libs and data

##Loading libs

The data table package is used to transform and filter data with high performance. NLP is used for wordcount. tm for loading docs and to transform to lower case. ngram to find the ngrams. ggplot and labling for the graphs. 

```{r}
library(data.table,  lib.loc="C:/TFS/Rlib/")
library(ngram,  lib.loc="C:/TFS/Rlib/")
library(reshape,  lib.loc="C:/TFS/Rlib/")
library(NLP,  lib.loc="C:/TFS/Rlib/")
library(tm,  lib.loc="C:/TFS/Rlib/")
library(labeling,  lib.loc="C:/TFS/Rlib/")
library(textcat,  lib.loc="C:/TFS/Rlib/")
library(ggplot2,  lib.loc="C:/TFS/Rlib/")

```

##Loading the data


```{r, eval=FALSE}
#Twitter
con <- file("files/final/en_US/en_US.twitter.txt", "r") 
txt = readLines(con, n=100) 
df_txt_twitter = data.frame(txt, stringsAsFactors = F) 
close(con)

#News
con <- file("files/final/en_US/en_US.news.txt", "r") 
txt = readLines(con, n=100) 
df_txt_news = data.frame(txt, stringsAsFactors = F) 
close(con)


#Blog
con <- file("files/final/en_US/en_US.blogs.txt", "r") 
txt = readLines(con, n=100) 
df_txt_blog = data.frame(txt, stringsAsFactors = F) 
close(con)
```

#Explore data

##View data

```{r}
#Twitter
head(df_txt_twitter, 3)

#News
head(df_txt_news, 3)

#Blog
head(df_txt_blog,3)
```

##Basic stats

Count_lines and Count_words function
```{r}
count_lines<-function(x) {
  dfr = data.frame(strsplit(x, "\\. |\\? "), stringsAsFactors = F)
  colnames(dfr) = "zin"
  zinnen = cbind(words = apply(dfr, 1, wordcount), dfr)
  length(zinnen$zin)
}

count_words<-function(x) {
  dfr = data.frame(strsplit(x, "\\. |\\? "), stringsAsFactors = F)
  colnames(dfr) = "zin"
  zinnen = cbind(words = apply(dfr, 1, wordcount), dfr)
  sum(zinnen$words)
}
```

###Total and average number of words Twitter

```{r}
l = sum(apply(head(df_txt_twitter, 3),1,count_lines))
w = sum(apply(head(df_txt_twitter, 3),1,count_words))
#lines
l
#words
w
#mean number of words:
w/l
```

###Total and average number of words Blogs

```{r}
l = sum(apply(head(df_txt_blog, 3),1,count_lines))
w = sum(apply(head(df_txt_blog, 3),1,count_words))
#lines
l
#words
w
#mean number of words:
w/l
```

###Total and average number of words News

```{r}
l = sum(apply(head(df_txt_news, 3),1,count_lines))
w = sum(apply(head(df_txt_news, 3),1,count_words))
#lines
l
#words
w
#mean number of words:
w/l
```

##Features of the data
```{r echo=F}
splitter<-function(x) {
  dfr = data.frame(strsplit(x, "\\. |\\? "), stringsAsFactors = F)
  colnames(dfr) = "zin"
  zinnen = cbind(words = apply(dfr, 1, wordcount), dfr)
  return(zinnen)
}

create_ngram_table = function(n, zinnen) {
  r = 1
  #create result data frame
  df <- data.frame(freq = integer(1000), w1 = character(1000),w2 = character(1000),
                   w3 = character(1000),w4 = character(1000),w5 = character(1000), stringsAsFactors = FALSE)
  #use n words sentences only
  nwords <- data.frame(zinnen[zinnen$words>n-1,2], stringsAsFactors = F)
  if (length(nwords[,]) > 0 ) {
    #loop trough sentences
    for (z in 1:length(nwords[,1])) { #for each sentence
      #get ngrams for sentence
      pt = data.frame(get.phrasetable(ngram(nwords[z,], n=n, sep=" ")),stringsAsFactors = F)
      #split in columns
      y = data.frame(transform(pt, pt = colsplit(ngrams, split = " ", names = rep('w', n))), stringsAsFactors = F)
      #convert new columns to character
      for (a in 1:n) {
        y[,a+3] = as.character(y[,a+3])  
      }
      
      #add each ngram to result data frame
      k = n+3
      l = n+1
      for (i in 1:length(y[,1])) {
        df[r,1] <- y[i,2]
        df[r,2:l] <- y[i,4:k]
        r <- r + 1
      }
      
    }
  }
  return(df)
}

loop_doc = function(a,d, df) {
    r = 1
    result <- data.frame(n = integer(2000000), w1 = character(2000000), w2 = character(2000000),
                         w3 = character(2000000), w4 = character(2000000), w5 = character(2000000),
                         stringsAsFactors = FALSE)
  #loop door de documenten op te bouwen
    for (i in 1:d) {
    str = df[i,]
    zinnen = splitter(str)
    xgram =create_ngram_table(a,zinnen)
    xgram = xgram[!xgram$w1 == "",]
    if (length(xgram[,1]) > 0 ) {
      result[r:(r+length(xgram[,1])-1),] = xgram[,1:(a+1)]
      r = r + length(xgram[,1])  
      }
  }
  
  result = result[!result$w1 == "",]
  result_dt = data.table(result)
  dtw = result_dt[, sum(n), by=c(names(result_dt)[2:(a+1)])]
  return(dtw)
}

loop_doc_lang = function(d) {
  r = 1
  result <- data.frame(zin = character(2000000), lang = character(2000000),
                       stringsAsFactors = FALSE)
  #loop door de documenten op te bouwen
  for (i in 1:d) {
    str = dataframe[1,]
    zinnen = splitter(str)
    xlang = data.frame(l = textcat(zinnen$zin), stringsAsFactors = F)
    
    if (length(xlang[,1]) > 0 ) {
      result[r:(r+length(xlang[,1])-1),1] = zinnen$zin
      result[r:(r+length(xlang[,1])-1),2] = xlang[,1]
      r = r + length(xlang[,1])  
    }
    #print(i)
  }
  result = result[!result$zin == "",]
  return(result)
}

```

###Frequency of singlegrams

```{r}
top50 =  function(n,df) {
  dfCorpus = Corpus(VectorSource(df))
  lower_case = tm_map(dfCorpus, content_transformer(tolower))
  
  dataframe<-data.frame(text=unlist(sapply(lower_case, `[`, "content")), 
                        stringsAsFactors=F)
  
  #Some words are more frequent than others - what are the distributions of word frequencies?
  q1 = loop_doc(n,100,dataframe)
  y = head(sort(q1$V1,decreasing=TRUE), n = 50)
  top50 = q1[q1$V1 %in% y,]
  top50 = top50[order(-V1),] 
  
  x = c(1:length(top50$w1))
  z = data.frame(cbind(x,top50))
  z
}
pt_df = top50(1,df_txt_twitter)
pt = ggplot(pt_df, aes(x, V1, label = w1)) + geom_text()
pt_df2 = top50(2,df_txt_twitter)
pt2 = pt + geom_line(data=pt_df2[1:50,], aes(x=x, y=V1), color='red')
pt2

pb_df = show_ngrams(1,df_txt_blog)
pb = ggplot(pb_df, aes(x, V1, label = w1)) + geom_text()
pb_df2 = top50(2,df_txt_blog)
pb2 = pb + geom_line(data=pb_df2[1:50,], aes(x=x, y=V1), color='red')
pb2

pn_df = show_ngrams(1,df_txt_news)
pn = ggplot(pn_df, aes(x, V1, label = w1)) + geom_text()
pn_df2 = top50(2,df_txt_news)
pn2 = pn + geom_line(data=pn_df2[1:50,], aes(x=x, y=V1), color='red')
pn2


```



When looking at these charts it becomes appearant that a small proportion of the words contribute to a large proportion of the words counts. Lets verify:

```{r}
q1 = loop_doc(1,100, df_txt_twitter)
q1 = q1[order(-V1),]

q1n = cbind(nr = c(1:length(q1$w1)), q1)
rt = sapply(1:length(q1$w1), function(x) sum(q1n[1:x]$V1))
q1n2 = cbind(q1n, rt)
t = sum(q1n$V1)
prc = rt / t * 100
prc = data.frame(pr = prc)
l = length(prc[prc$pr < 50.1,]) / length(prc[,]) * 100
l
```

We only need `l` procent of the words to get to 50% of the  frequency count. Lets see how much we need to get to 90%:

```{r}
l = length(prc[prc$pr < 90.1,]) / length(prc[,]) * 100
l
```

The y axis shows the percentage of words the x axis shows the percentage of the word count. 


```{r}
dt = data.table(round(prc$pr))
df = data.frame(dt[, .N, by=V1])
rt = sapply(1:length(df$V1), function(x) sum(df[1:x,]$N))
df2 = cbind(df, wf= rt)
#prop of words
pw = round(rt / max(rt) * 100)
df2 = cbind(df2, pw)
curve = ggplot(df2, aes(x=V1, y=pw)) + geom_line()
curve
```
