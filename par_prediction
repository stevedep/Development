install.packages("doParallel",dependencies=TRUE)
install.packages("ngram",dependencies=TRUE)
install.packages("reshape",dependencies=TRUE)
install.packages("doMC",dependencies=TRUE)
install.packages("data.table",dependencies=TRUE)

library(data.table)
library(doParallel)
library(ngram)
library(reshape)
library(foreach)
library(doMC)

#download file
url = "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip" 
destfile = "Coursera-SwiftKey.zip"
download.file(url, destfile)

#unzip
unzip(destfile)

#functions
splitter<-function(x) {
  dfr = data.frame(strsplit(x, "\\. |\\? "), stringsAsFactors = F)
  colnames(dfr) = "zin"
  zinnen = cbind(words = apply(dfr, 1, wordcount), dfr)
  return(zinnen)
}

create_ngram_table = function(n, zinnen) {
  r = 1
  #create result data frame
  df <- data.frame(freq = integer(1000), w1 = character(1000),w2 = character(1000),
                   w3 = character(1000),w4 = character(1000),w5 = character(1000), stringsAsFactors = FALSE)
  #use n words sentences only
  nwords <- data.frame(zinnen[zinnen$words>n-1,2], stringsAsFactors = F)
  if (length(nwords[,]) > 0 ) {
    #loop trough sentences
    for (z in 1:length(nwords[,1])) { #for each sentence
      #get ngrams for sentence
      pt = data.frame(get.phrasetable(ngram(nwords[z,], n=n, sep=" ")),stringsAsFactors = F)
      #split in columns
      y = data.frame(transform(pt, pt = colsplit(ngrams, split = " ", names = rep('w', n))), stringsAsFactors = F)
      #convert new columns to character
      for (a in 1:n) {
        y[,a+3] = as.character(y[,a+3])  
      }
      
      #add each ngram to result data frame
      k = n+3
      l = n+1
      for (i in 1:length(y[,1])) {
        df[r,1] <- y[i,2]
        df[r,2:l] <- y[i,4:k]
        r <- r + 1
      }
      
    }
  }
  return(df)
}

f_create_ngram = function (nl, ds,n, parts) {#number of lines, dataset, # ngram 
  
  intm_result = parLapply(cl, ds[,],
                          function(x) {
                            library(ngram)
                            library(reshape)
                            zinnen = splitter(x)
                            xgram =create_ngram_table(n,zinnen)
                            xgram = xgram[!xgram$w1 == "",]
                            xgram
                          }
  )
  
  result = foreach(j=seq(1,nl, by=(nl/parts))) %dopar% {
    t = 0
    for (i in j:(j+(nl/parts-1))) {
      t = t + length(intm_result[[i]]$w1)
    }
    
    result <- data.frame(n = integer(t), w1 = character(t), w2 = character(t),
                         w3 = character(t), w4 = character(t), w5 = character(t),
                         stringsAsFactors = FALSE)
    r=1
    for (i in j:(j+(nl/parts-1))) {
      if (length(intm_result[[i]][,1]) > 0 ) {
        result[r:(r+length(intm_result[[i]][,1])-1),] = intm_result[[i]][,]
        r = r + length(intm_result[[i]][,1])  
      }
    }
    result
  }  
  do.call("rbind", result)
}

topnperc =  function(df, p,a) { #data frame, percentage, #ngram
  dt = data.table(df)
  dt_a = dt[, sum(n), by=c(names(dt)[2:(a+1)])]
  n = length(dt_a$V1) / 100 * p
  y = head(sort(dt_a$V1,decreasing=TRUE), n)
  topn = dt_a[dt_a$V1 %in% y,]
  topn = topn[order(-V1),] 
  
  x = c(1:length(topn$w1))
  z = data.frame(cbind(x,topn))
  z
}


##Start processing singlegram
#load twitter
nl = 100000

#Twitter
con <- file("final/en_US/en_US.twitter.txt", "r") 
txt = readLines(con, n=nl) 
close(con)
df_txt_twitter = data.frame(txt, stringsAsFactors = F) 

#prep parralel processing
# Calculate the number of cores
no_cores <- detectCores() - 1
# Initiate cluster
cl <- makeCluster(no_cores)
registerDoMC(no_cores) 
clusterExport(cl=cl, varlist=c("splitter", "create_ngram_table"))


singlegram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 1, parts = 39)
twogram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 2, parts = 39)
threegram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 3, parts = 39)
fourgram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 4, parts = 39)
fivegram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 5, parts = 39)
system.time(f_create_ngram(nl,ds = df_txt_twitter,n = 1, parts = 78))


remove(top25p_singlegram)
top18p_singlegram = topnperc(singlegram_twitter,18,1)
top18p_twogram = topnperc(twogram_twitter,18,1)
top18p_threegram = topnperc(threegram_twitter,18,1)


stopCluster(cl)
