#install.packages("doParallel",dependencies=TRUE)
library(doParallel)
#install.packages("ngram",dependencies=TRUE)
library(ngram)
#install.packages("reshape",dependencies=TRUE)
library(reshape)
library(foreach)
#install.packages("doMC",dependencies=TRUE)
library(doMC)

#download file
url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip" 
destfile = "Coursera-SwiftKey.zip"
download.file(url, destfile)

#unzip
unzip(destfile)

#functions
splitter<-function(x) {
  dfr = data.frame(strsplit(x, "\\. |\\? "), stringsAsFactors = F)
  colnames(dfr) = "zin"
  zinnen = cbind(words = apply(dfr, 1, wordcount), dfr)
  return(zinnen)
}

create_ngram_table = function(n, zinnen) {
  r = 1
  #create result data frame
  df <- data.frame(freq = integer(1000), w1 = character(1000),w2 = character(1000),
                   w3 = character(1000),w4 = character(1000),w5 = character(1000), stringsAsFactors = FALSE)
  #use n words sentences only
  nwords <- data.frame(zinnen[zinnen$words>n-1,2], stringsAsFactors = F)
  if (length(nwords[,]) > 0 ) {
    #loop trough sentences
    for (z in 1:length(nwords[,1])) { #for each sentence
      #get ngrams for sentence
      pt = data.frame(get.phrasetable(ngram(nwords[z,], n=n, sep=" ")),stringsAsFactors = F)
      #split in columns
      y = data.frame(transform(pt, pt = colsplit(ngrams, split = " ", names = rep('w', n))), stringsAsFactors = F)
      #convert new columns to character
      for (a in 1:n) {
        y[,a+3] = as.character(y[,a+3])  
      }
      
      #add each ngram to result data frame
      k = n+3
      l = n+1
      for (i in 1:length(y[,1])) {
        df[r,1] <- y[i,2]
        df[r,2:l] <- y[i,4:k]
        r <- r + 1
      }
      
    }
  }
  return(df)
}

f_create_ngram = function (nl, ds,n) {#number of lines, dataset, # ngram 

  intm_result = parLapply(cl, ds[,],
                          function(x) {
                            library(ngram)
                            library(reshape)
                            zinnen = splitter(x)
                            xgram =create_ngram_table(n,zinnen)
                            xgram = xgram[!xgram$w1 == "",]
                            xgram
                          }
  )
  
  result = foreach(j=seq(1,nl, by=(nl/10))) %dopar% {
    t = 0
    for (i in j:(j+(nl/10-1))) {
      t = t + length(intm_result[[i]]$w1)
    }
    
    result <- data.frame(n = integer(t), w1 = character(t), w2 = character(t),
                         w3 = character(t), w4 = character(t), w5 = character(t),
                         stringsAsFactors = FALSE)
    r=1
    for (i in j:(j+(nl/10-1))) {
      if (length(intm_result[[i]][,1]) > 0 ) {
        result[r:(r+length(intm_result[[i]][,1])-1),] = intm_result[[i]][,]
        r = r + length(intm_result[[i]][,1])  
      }
    }
    result
  }  
  do.call("rbind", result)
}

##Start processing singlegram
#load twitter
nl = 1000000

#Twitter
con <- file("final/en_US/en_US.twitter.txt", "r") 
txt = readLines(con, n=nl) 
close(con)
df_txt_twitter = data.frame(txt, stringsAsFactors = F) 

#prep parralel processing
# Calculate the number of cores
no_cores <- detectCores() - 1
# Initiate cluster
cl <- makeCluster(no_cores)
registerDoMC(no_cores) 
clusterExport(cl=cl, varlist=c("splitter", "create_ngram_table"))


singlegram_twitter = f_create_ngram(nl,ds = df_txt_twitter,n = 1)

stopCluster(cl)
